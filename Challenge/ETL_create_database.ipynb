{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from config import postgres_pwd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Movie\n",
    "alt_langs = ['Also known as','Arabic','Cantonese','Chinese','French','Hangul','Hebrew','Hepburn','Japanese',\n",
    "             'Literally','Mandarin','McCune–Reischauer','Original title','Polish','Revised Romanization','Romanized',\n",
    "             'Russian','Simplified','Traditional','Yiddish']\n",
    "def clean_movie(x):\n",
    "    x = dict(x)\n",
    "    \n",
    "    # Consolidate Titles from all Languages\n",
    "    alt_titles = {}\n",
    "    for i in alt_langs:\n",
    "        if i in x:\n",
    "            alt_titles[i] = x[i]\n",
    "            x.pop(i)\n",
    "    if alt_titles:\n",
    "        x['Alternate Title'] = alt_titles\n",
    "    \n",
    "    # Consolidate similar Columns\n",
    "    def consolidate_columns(a,b):\n",
    "        if a in x:\n",
    "            x[b] = x.pop(a)\n",
    "    consolidate_columns('Adaptation by', 'Writer(s)')\n",
    "    consolidate_columns('Written by', 'Writer(s)')\n",
    "    consolidate_columns('Screen story by', 'Writer(s)')\n",
    "    consolidate_columns('Screenplay by', 'Writer(s)')\n",
    "    consolidate_columns('Story by', 'Writer(s)')\n",
    "    consolidate_columns('Country of origin', 'Country')\n",
    "    consolidate_columns('Directed by', 'Director')\n",
    "    consolidate_columns('Distributed by', 'Distributor')\n",
    "    consolidate_columns('Edited by', 'Editor(s)')\n",
    "    consolidate_columns('Length', 'Running time')\n",
    "    consolidate_columns('Music by', 'Composer(s)')\n",
    "    consolidate_columns('Theme music composer', 'Composer(s)')\n",
    "    consolidate_columns('Produced by', 'Producer(s)')\n",
    "    consolidate_columns('Producer', 'Producer(s)')\n",
    "    consolidate_columns('Productioncompanies ', 'Production company(s)')\n",
    "    consolidate_columns('Productioncompany ', 'Production company(s)')\n",
    "    consolidate_columns('Original release', 'Release date')\n",
    "    consolidate_columns('Released', 'Release date')\n",
    "    consolidate_columns('Release Date', 'Release date')\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract, Transform and Load\n",
    "def extract_transform_load(wiki,kaggle,ratings):\n",
    "\n",
    "    # Extract Wiki Data\n",
    "    with open(wiki_file,'r') as wiki_json:\n",
    "        wiki_list = json.load(wiki_json)\n",
    "        wiki_list = [i for i in wiki_list \n",
    "                     if ('Director' in i or 'Directed by' in i)\n",
    "                       and 'imdb_link' in i\n",
    "                       and 'No. of episodes' not in i\n",
    "                    ]\n",
    "        wiki_clean_list = [clean_movie(i) for i in wiki_list]\n",
    "        wiki_df = pd.DataFrame(wiki_clean_list)\n",
    "\n",
    "    # Transform Wiki Data - Extract IMDB ID    \n",
    "    try:       \n",
    "        wiki_df['imdb_id'] = wiki_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "        wiki_df.drop_duplicates(subset='imdb_id',inplace=True)\n",
    "    except:\n",
    "        print('Error')\n",
    "\n",
    "    # Transform Wiki Data - Drop Columns with Null values in 90% of rows\n",
    "    wiki_columns_to_keep = [i for i in wiki_df if wiki_df[i].count() >= (len(wiki_df)*0.1)]\n",
    "    wiki_df = wiki_df[wiki_columns_to_keep]\n",
    "\n",
    "    # Transform Wiki Data - Clean Box Office\n",
    "    box_off = wiki_df['Box office'].dropna() \n",
    "    box_off = box_off.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    box_off = box_off.str.replace(r'\\$.*[-—–](?![a-z])','$', regex=True)\n",
    "    format_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    format_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illi?on)'\n",
    "    def parse_dollars(x):\n",
    "        if type(x) != str:\n",
    "            return np.nan\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on',x,flags=re.IGNORECASE):\n",
    "            x = re.sub('\\$|[a-zA-Z]|\\s','',x)\n",
    "            x = float(x) * 10**6\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on',x,flags=re.IGNORECASE):\n",
    "            x = re.sub('\\$|[a-zA-Z]|\\s','',x)\n",
    "            x = float(x) * 10**9\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illi?on)',x,flags=re.IGNORECASE):\n",
    "            x = re.sub('\\$|,','',x)\n",
    "            x = float(x)\n",
    "        else:\n",
    "            return np.nan\n",
    "        return x     \n",
    "    wiki_df['box_office'] = box_off.str.extract(f'({format_one}|{format_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_df.drop('Box office', axis=1, inplace=True)\n",
    "\n",
    "    # Transform Wiki Data - Clean Budget\n",
    "    bud = wiki_df['Budget'].dropna()\n",
    "    bud = bud.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    bud = bud.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "    bud = bud.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    format_one = r'\\$\\s*\\d+\\.?\\d*\\s*mill?i?on'\n",
    "    format_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\smillion)'\n",
    "    wiki_df['budget'] = bud.str.extract(f'({format_one}|{format_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_df.drop('Budget', axis=1, inplace=True)\n",
    "\n",
    "    # Transform Wiki Data - Clean Release Date\n",
    "    rel_dt = wiki_df['Release date'].dropna()\n",
    "    rel_dt = rel_dt.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    month = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)'\n",
    "    format_one = month + r'\\s\\d{1,2}\\,\\s\\d{4}'\n",
    "    format_two = r'(?:\\d{1,2}\\s)?' + month + r' \\d{4}'\n",
    "    format_three = '(' + month + ',\\s\\d{4})'\n",
    "    format_four = r'\\d{4}.[01]\\d.[0123]\\d'\n",
    "    format_five = r'(^\\d{4})'\n",
    "    wiki_df['release_date'] = pd.to_datetime(rel_dt.str.extract(f'({format_one}|{format_two}|{format_three}|{format_four}|{format_five})', flags=re.IGNORECASE)[0]) \n",
    "    wiki_df.drop('Release date', axis=1, inplace=True)\n",
    "\n",
    "    # Transform Wiki Data - Clean Run Time\n",
    "    run = wiki_df['Running time'].dropna()\n",
    "    run = run.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    format_one = r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)'\n",
    "    format_two = r'^(\\d+)\\s*m?'\n",
    "    run_time_extract = run.str.extract(f'({format_one}|{format_two})', flags=re.IGNORECASE)\n",
    "    run_time_extract = run_time_extract.apply(lambda x: pd.to_numeric(x,errors='coerce')).fillna(0)\n",
    "    wiki_df['running_time'] = run_time_extract.apply(lambda x: x[1]*60+x[2] if x[2]!=0 else x[3], axis=1)\n",
    "    wiki_df.drop('Running time', axis=1, inplace=True)\n",
    "\n",
    "    # Extract & Transform Kaggle Data\n",
    "    kaggle_df = pd.read_csv(kaggle_file)\n",
    "    kaggle_df = kaggle_df.loc[kaggle_df['adult'] == 'False'].drop(['adult'],axis=1)\n",
    "    kaggle_df['video'] = kaggle_df['video'] == True\n",
    "    kaggle_df['budget'] = pd.to_numeric(kaggle_df['budget'], errors='raise')\n",
    "    kaggle_df['id'] = pd.to_numeric(kaggle_df['id'], errors='raise')\n",
    "    kaggle_df['popularity'] = pd.to_numeric(kaggle_df['popularity'], errors='raise')\n",
    "    kaggle_df['release_date'] = pd.to_datetime(kaggle_df['release_date'])\n",
    "\n",
    "    # Merge Wiki and Kaggle into Movies DataFrame\n",
    "    movies_df = pd.merge(wiki_df, kaggle_df, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n",
    "    # Clean Movies DF\n",
    "    movies_df = movies_df[movies_df['id']!=11426] # Deleting this record because of title mismatch between Wiki/Kaggle\n",
    "    def fill_missing_data(df,source,destination):\n",
    "        df[destination] = df.apply(lambda x: x[source] if x[destination]==0 else x[destination],axis=1)\n",
    "    fill_missing_data(movies_df,'running_time','runtime')\n",
    "    fill_missing_data(movies_df,'budget_wiki','budget_kaggle')\n",
    "    fill_missing_data(movies_df,'box_office','revenue')\n",
    "    movies_df = movies_df.loc[:, [\n",
    "        'imdb_id',\n",
    "        'id',\n",
    "        'title_kaggle',\n",
    "        'original_title',\n",
    "        'tagline',\n",
    "        'belongs_to_collection',\n",
    "        'url',\n",
    "        'imdb_link',\n",
    "        'runtime',\n",
    "        'budget_kaggle',\n",
    "        'revenue',\n",
    "        'release_date_kaggle',\n",
    "        'popularity',\n",
    "        'vote_average',\n",
    "        'vote_count',\n",
    "        'genres',\n",
    "        'original_language',\n",
    "        'overview',\n",
    "        'spoken_languages',\n",
    "        'Country',\n",
    "        'production_companies',\n",
    "        'production_countries',\n",
    "        'Distributor',\n",
    "        'Producer(s)',\n",
    "        'Director',\n",
    "        'Starring',\n",
    "        'Cinematography',\n",
    "        'Editor(s)',\n",
    "        'Writer(s)',\n",
    "        'Composer(s)',\n",
    "        'Based on'\n",
    "        ]]\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                      'title_kaggle':'title',\n",
    "                      'url':'wikipedia_url',\n",
    "                      'budget_kaggle':'budget',\n",
    "                      'release_date_kaggle':'release_date',\n",
    "                      'Country':'country',\n",
    "                      'Distributor':'distributor',\n",
    "                      'Producer(s)':'producers',\n",
    "                      'Director':'director',\n",
    "                      'Starring':'starring',\n",
    "                      'Cinematography':'cinematography',\n",
    "                      'Editor(s)':'editors',\n",
    "                      'Writer(s)':'writers',\n",
    "                      'Composer(s)':'composers',\n",
    "                      'Based on':'based_on'\n",
    "                     },\n",
    "                     axis='columns',\n",
    "                     inplace=True)\n",
    "\n",
    "    # Extract & Transform Ratings Data\n",
    "    ratings_df = pd.read_csv(ratings_file)\n",
    "    rating_counts = ratings_df.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                    .rename({'userId':'count'}, axis=1) \\\n",
    "                    .pivot(index='movieId',columns='rating', values='count')    \n",
    "    rating_counts.columns = ['rating_' + str(i) for i in rating_counts.columns]\n",
    "    \n",
    "    # Merge Movies and Ratings Data\n",
    "    movies_with_ratings_df = pd.merge(\n",
    "                                    movies_df,\n",
    "                                    rating_counts,\n",
    "                                    how='left',\n",
    "                                    left_on='kaggle_id',\n",
    "                                    right_index=True\n",
    "                                )\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "    \n",
    "    # Establish Database Connection\n",
    "    db_string = f\"postgres://postgres:{postgres_pwd}@localhost:5432/movie_data\"\n",
    "    engine = create_engine(db_string)\n",
    "    \n",
    "    # Create SQL Tables\n",
    "    movies_df.to_sql(name='movies', con=engine, if_exists='replace')\n",
    "    rows_imported = 0\n",
    "    start_time = time.time()\n",
    "    for data in pd.read_csv(ratings_file, chunksize=1000000):\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "        rows_imported += len(data)\n",
    "        print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign file paths\n",
    "file_dir = '../Data/'\n",
    "wiki_file = f'{file_dir}wikipedia-movies.json'\n",
    "kaggle_file = f'{file_dir}movies_metadata.csv'\n",
    "ratings_file = f'{file_dir}ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/PythonData/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...Done. 437.17898201942444 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 770.2851700782776 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 1123.565827846527 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 1487.373540878296 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 1893.2138838768005 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 2273.2231121063232 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 2639.6773982048035 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 3085.395299911499 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 3518.668941259384 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 3943.8692231178284 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 4261.564292907715 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 4641.88893198967 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 5010.336407184601 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 5374.959090948105 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 5757.392231941223 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 6136.959415197372 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 6617.686069011688 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 7027.698920965195 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 7449.224021196365 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 7798.049890041351 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done. 8209.68481206894 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done. 8613.932605028152 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done. 8828.34574007988 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done. 8992.113983154297 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done. 9187.139461040497 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done. 9356.602354049683 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done. 9362.489858865738 total seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "# Run ETL Function\n",
    "extract_transform_load(wiki_file,kaggle_file,ratings_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
